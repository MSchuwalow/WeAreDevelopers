{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dense, Dropout, Flatten\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "Word2Vec_PATH = '../resources/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors. This will take a while...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Load the Word2Vec data. We'll need this later\n",
    "\n",
    "class Embeddings:\n",
    "    \"\"\"Loads and manages the Word2vec Data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        print('Loading word vectors. This will take a while...')\n",
    "        self.data = KeyedVectors.load_word2vec_format(Word2Vec_PATH, binary=True)\n",
    "        print('done.')\n",
    "\n",
    "    def lookup(self, word):\n",
    "        \"\"\"\n",
    "        return the vector representation of a word. falls back to a zero vector if no match was found\n",
    "        :param word: the word to lookup\n",
    "        :return: the vector representation\n",
    "        \"\"\"\n",
    "        if word == '//pad//':\n",
    "            return [0 for i in range(300)]\n",
    "        try:\n",
    "            return np.asarray(self.data.word_vec(word))\n",
    "        # Replace words not found in vocabulary with zeros\n",
    "        except KeyError:\n",
    "            return np.zeros(300)\n",
    "    \n",
    "emb = Embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector for cat is: [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
      " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
      " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
      " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
      " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
      " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
      "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
      " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
      "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
      "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
      "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
      " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
      "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
      " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
      " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
      "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
      "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
      "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
      " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
      " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
      " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
      " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
      "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
      "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
      "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
      "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
      "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
      "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
      " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
      " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
      "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
      "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
      " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
      " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
      " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
      "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
      " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
      "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
      " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
      "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
      "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
      " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
      " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
      "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
      " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
      " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
      " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
      "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
      "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n"
     ]
    }
   ],
   "source": [
    "# Test the lookup\n",
    "\n",
    "print(\"The vector for cat is: %s\" % emb.lookup(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now lets load the dataset. We are going to use the imbd dataset.\n",
    "\"\"\"\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 25000\n",
      "testing data: 25000\n",
      "number of unique words: 88585\n",
      "average length of review: 238\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at our data\n",
    "print(\"training data: %s\" % np.shape(x_train))\n",
    "print(\"testing data: %s\" % np.shape(x_test))\n",
    "\n",
    "print(\"number of unique words: %d\" % len(np.unique(np.hstack(x_train + x_test))))\n",
    "print(\"average length of review: %d\" % np.average([len(x) for x in x_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              251000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 477,251\n",
      "Trainable params: 477,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now lets construct our model. We are going to use a convolutional layer with 250 filters of size 3.\n",
    "This is followed by a MaxPool layer, Dropout and finally one densely\n",
    "connected layer.\n",
    "For our output we'll use a normal softmax.\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "Now we add the convolutional layer. Keras expects us to provide a shape for the first layer.\n",
    "We'll use sequences of length 400 and of dimensionality 300 (innate to word2vec).\n",
    "Further we'll use relu as the activation funcition.\n",
    "\"\"\"\n",
    "model.add(Conv1D(filters=250, kernel_size=3, input_shape=(400, 300), activation=\"relu\"))\n",
    "\"\"\"\n",
    "Now we'll apply a max pooling layer with size 5\n",
    "\"\"\"\n",
    "model.add(GlobalMaxPool1D())\n",
    "# one dense layer with 500 neurons\n",
    "model.add(Dense(1000))\n",
    "# Now some dropout\n",
    "model.add(Dropout(0.2))\n",
    "# and our output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# finally our optimizer. we are going with adam here. Note that this is a binary classification dataset.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 141s - loss: 0.4733 - acc: 0.7647   - ETA: 2s - loss\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 140s - loss: 0.2934 - acc: 0.8750   \n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 140s - loss: 0.2055 - acc: 0.9164   \n",
      "Accuracy: 84.42%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset is provided as indexes in a dict of words. If we want to use our word embeddings we have\n",
    "to first transform the indexes back to normal text. Then we can look them up in our embeddings\n",
    "\"\"\"\n",
    "def embed(data, length):\n",
    "    tokens = [inv_dict.get(index) for index in data]\n",
    "    tokens += ['//pad//'] * (length - len(tokens))\n",
    "    return np.asarray([emb.lookup(word) for word in tokens[:length]])\n",
    "    \n",
    "\"\"\"\n",
    "As the dataset becomes quite big once we transform into its word embedded form we are going to define a generator here.\n",
    "We are also going to pad/cut the texts to a fixed length of 400 words.\n",
    "\"\"\"\n",
    "\n",
    "inv_dict = {value: key for key, value in imdb.get_word_index().items()}\n",
    "def data_generator(data, target, length=400, num=20):\n",
    "    while True:\n",
    "        for i in range(0, len(data), num):\n",
    "            texts = np.asarray([embed(entry, length) for entry in data[i:i+num]])\n",
    "            results = np.asarray(target[i:i+num])\n",
    "            yield (texts, results)\n",
    "\n",
    "\n",
    "# now lets train our model\n",
    "model.fit_generator(data_generator(x_train, y_train), steps_per_epoch=1250, epochs=3, max_q_size=20)\n",
    "\n",
    "\n",
    "# and test its performance\n",
    "scores = model.evaluate_generator(data_generator(x_test, y_test), 1250, max_q_size=20)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
